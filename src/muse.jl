



### MUSE result

@doc doc"""

Stores the result of a MUSE run. Can be constructed by-hand as
`MuseResult()` and passed to any of the inplace `muse!`, `get_J!`, or
`get_H!`.

Fields:

* `Î¸` â€” The estimate of the $\theta$ parameters. 
* `Î£, Î£â»Â¹` â€” The approximate covariance of $\theta$ and its inverse. 
* `H, J` â€” The $H$ and $J$ matrices which form the covariance (see
  [Millea & Seljak, 2021](https://arxiv.org/abs/2112.09354))
* `gs` â€” The MAP gradient sims used to compute `J`.
* `Hs` â€” The jacobian sims used to compute `H`. 
* `dist` â€” A `Normal` or `MvNormal` built from `Î¸` and `Î£`, for
  convenience. 
* `history` â€” Internal diagnostic info from the run. 
* `rng` â€” RNG used to generate sims for this run (so the same sims can
  be reused if resuming later).
* `time` â€” Total `Millisecond` wall-time spent computing the result.

"""
Base.@kwdef mutable struct MuseResult
    Î¸ = nothing
    H = nothing
    J = nothing
    Î£â»Â¹ = nothing
    Î£ = nothing
    dist = nothing
    history = []
    gs = []
    Hs = []
    metadata = Dict()
    rng = nothing
    time = Millisecond(0)
end


function Base.show(io::IO, result::MuseResult)
    _print(Î¼) = @sprintf("%.4g", Î¼)
    _print(Î¼, Ïƒ) = @sprintf("%.4gÂ±%.3g", Î¼, Ïƒ)
    print(io, "MuseResult(")
    if result.Î¸ != nothing && result.Î£ != nothing
        ÏƒÂ² = result.Î¸ isa AbstractVector ? diag(result.Î£) : result.Î£
        str = sprint(show, _print.(result.Î¸, sqrt.(ÏƒÂ²)))
    elseif result.Î¸ != nothing
        str = sprint(show, _print.(result.Î¸))
    else
        str = ""
    end
    print(io, replace(str, "\"" => ""))
    print(io,")")
end

### MUSE solver

@doc doc"""

    muse(prob::AbstractMuseProblem, Î¸â‚€; kwargs...)
    muse!(result::MuseResult, prob::AbstractMuseProblem, [Î¸â‚€=nothing]; kwargs...)

Run the MUSE estimate. The `muse!` form resumes an existing result. If
the `muse` form is used instead, `Î¸â‚€` must give a starting guess for
$\theta$.

See [`MuseResult`](@ref) for description of return value. 

Keyword arguments:

* `rng` â€” Random number generator to use. Taken from `result.rng` or
  `Random.default_rng()` if not passed. 
* `zâ‚€` â€” Starting guess for the latent space MAP.
* `maxsteps = 50` â€” Maximum number of iterations. 
* `Î¸_rtol = 1e-2` â€” Error tolerance on $\theta$ relative to its
  standard deviation.
* `âˆ‡z_logLike_atol = 1e-2` â€” Absolute tolerance on the $z$-gradient at
  the MAP solution. 
* `nsims = 100` â€” Number of simulations. 
* `Î± = 0.7` â€” Step size for root-finder. 
* `progress = false` â€” Show progress bar.
* `pool :: AbstractWorkerPool` â€” Worker pool for parallelization.
* `regularize = identity` â€” Apply some regularization after each step. 
* `Hâ»Â¹_like = nothing` â€” Initial guess for the inverse Jacobian of
  $s^{\rm MUSE}(\theta)$
* `Hâ»Â¹_update` â€” How to update `Hâ»Â¹_like`. Should be `:sims`,
  `:broyden`, or `:diagonal_broyden`. 
* `broyden_memory = Inf` â€” How many past steps to keep for Broyden
  updates. 
* `checkpoint_filename = nothing` â€” Save result to a file after each
  iteration. 
* `get_covariance = false` â€” Also call `get_H` and `get_J` to get the
  full covariance.

"""
muse(args...; kwargs...) = muse!(MuseResult(), args...; kwargs...)

@doc doc"""
See [`muse`](@ref).
"""
function muse!(
    result :: MuseResult,
    prob :: AbstractMuseProblem, 
    Î¸â‚€ = nothing;
    rng = nothing,
    zâ‚€ = nothing,
    maxsteps = 50,
    Î¸_rtol = 1e-1,
    âˆ‡z_logLike_atol = 1e-2,
    nsims = 100,
    Î± = 0.7,
    progress = false,
    pool = LocalWorkerPool(),
    regularize = identity,
    Hâ»Â¹_likeâ€² = nothing,
    Hâ»Â¹_update = :sims,
    broyden_memory = Inf,
    checkpoint_filename = nothing,
    get_covariance = false
)

    result.rng = rng = @something(rng, result.rng, copy(Random.default_rng()))
    Î¸unreg  = Î¸  = Î¸â‚€ = standardizeÎ¸(prob, @something(result.Î¸, Î¸â‚€))
    Î¸unregâ€² = Î¸â€² = transform_Î¸(prob, Î¸)
    history = result.history
    
    _rng = copy(rng)
    xs_zÌ‚s_sims = pmap(pool, 1:nsims) do i
        (x, z) = sample_x_z(prob, _rng, Î¸)
        (x, @something(zâ‚€, z))
    end
    xs = [[prob.x];                                      first.(xs_zÌ‚s_sims)]
    zÌ‚s = [[@something(zâ‚€, sample_x_z(prob, _rng, Î¸).z)]; last.(xs_zÌ‚s_sims)]

    # set up progress bar
    pbar = progress ? RemoteProgress((maxsteps-length(result.history))*(nsims+1), 0.1, "MUSE: ") : nothing

    try
    
        for i = (length(result.history)+1):maxsteps
            
            tâ‚€ = now()

            if i > 1
                _rng = copy(rng)
                xs = [[prob.x]; pmap(pool, 1:nsims) do
                    sample_x_z(prob, _rng, Î¸).x
                end]
            end

            if i > 2
                Î”Î¸â€² = history[end].Î¸â€² - history[end-1].Î¸â€²
                sqrt(-(Î”Î¸â€²' * history[end].Hâ»Â¹_postâ€² * Î”Î¸â€²)) < Î¸_rtol && break
            end

            # MUSE gradient
            gzÌ‚s = pmap(pool, xs, zÌ‚s, fill(Î¸,length(xs))) do x, zÌ‚_prev, Î¸
                local zÌ‚, history = zÌ‚_at_Î¸(prob, x, zÌ‚_prev, Î¸; âˆ‡z_logLike_atol)
                g  = âˆ‡Î¸_logLike(prob, x, zÌ‚, Î¸,  UnTransformedÎ¸())
                gâ€² = âˆ‡Î¸_logLike(prob, x, zÌ‚, Î¸â€², TransformedÎ¸())
                progress && ProgressMeter.next!(pbar)
                (;g, gâ€², zÌ‚, history)
            end
            zÌ‚s = getindex.(gzÌ‚s, :zÌ‚)
            zÌ‚_history_dat, zÌ‚_history_sims = peel(getindex.(gzÌ‚s, :history))
            g_like_datâ€², g_like_simsâ€² = peel(getindex.(gzÌ‚s, :gâ€²))
            _,           g_like_sims  = peel(getindex.(gzÌ‚s, :g))
            g_likeâ€² = g_like_datâ€² .- mean(g_like_simsâ€²)
            g_priorâ€² = AD.gradient(AD.ForwardDiffBackend(), Î¸â€² -> logPriorÎ¸(prob, Î¸â€², TransformedÎ¸()), Î¸â€²)[1]
            g_postâ€² = g_likeâ€² .+ g_priorâ€²

            # Jacobian
            hâ»Â¹_like_simsâ€² = -1 ./ var(collect(g_like_simsâ€²))
            Hâ»Â¹_like_simsâ€² = hâ»Â¹_like_simsâ€² isa Number ? hâ»Â¹_like_simsâ€² : Diagonal(hâ»Â¹_like_simsâ€²)
            if (Hâ»Â¹_likeâ€² == nothing) || (Hâ»Â¹_update == :sims)
                Hâ»Â¹_likeâ€² = Hâ»Â¹_like_simsâ€²
            elseif i > 2 && (Hâ»Â¹_update in [:broyden, :diagonal_broyden])
                # on subsequent steps, do a Broyden's update using at
                # most the previous `broyden_memory` steps
                jâ‚€ = Int(max(2, i - broyden_memory))
                Hâ»Â¹_likeâ€² = history[jâ‚€-1].Hâ»Â¹_like_simsâ€²
                for j = jâ‚€:i-1
                    Î”Î¸â€²      = history[j].Î¸â€²      - history[j-1].Î¸â€²
                    Î”g_likeâ€² = history[j].g_likeâ€² - history[j-1].g_likeâ€²
                    Hâ»Â¹_likeâ€² = Hâ»Â¹_likeâ€² + ((Î”Î¸â€² - Hâ»Â¹_likeâ€² * Î”g_likeâ€²) / (Î”Î¸â€²' * Hâ»Â¹_likeâ€² * Î”g_likeâ€²)) * Î”Î¸â€²' * Hâ»Â¹_likeâ€²
                    if Hâ»Â¹_update == :diagonal_broyden
                        Hâ»Â¹_likeâ€² = Diagonal(Hâ»Â¹_likeâ€²)
                    end
                end
            end

            H_priorâ€² = AD.hessian(AD.ForwardDiffBackend(), Î¸â€² -> logPriorÎ¸(prob, Î¸â€², TransformedÎ¸()), Î¸â€²)[1]
            Hâ»Â¹_postâ€² = inv(inv(Hâ»Â¹_likeâ€²) + H_priorâ€²)

            t = now() - tâ‚€
            push!(
                history, 
                (;
                    Î¸, Î¸unreg, Î¸â€², Î¸unregâ€²,
                    g_like_sims,
                    g_like_datâ€², g_like_simsâ€², g_likeâ€², g_priorâ€², g_postâ€², 
                    Hâ»Â¹_postâ€², H_priorâ€², Hâ»Â¹_likeâ€², Hâ»Â¹_like_simsâ€², 
                    zÌ‚_history_dat, zÌ‚_history_sims, t
                )
            )

            # Newton-Rhapson step
            Î¸unregâ€² = Î¸â€² .- Î± .* (Hâ»Â¹_postâ€² * g_postâ€²)
            Î¸unreg  = inv_transform_Î¸(prob, Î¸unregâ€²)
            Î¸â€² = regularize(Î¸unregâ€²)
            Î¸  = inv_transform_Î¸(prob, Î¸â€²)

            (checkpoint_filename != nothing) && save(checkpoint_filename, "result", result)

        end

    finally

        progress && ProgressMeter.finish!(pbar)
        
    end
    
    result.time += sum(getindex.(history,:t))
    result.Î¸ = Î¸unreg
    result.gs = collect(history[end].g_like_sims)
    if get_covariance
        get_J!(result, prob; rng, nsims, âˆ‡z_logLike_atol)
        get_H!(result, prob; rng, nsims=max(1,nsimsÃ·10), âˆ‡z_logLike_atol)
    end
    result

end


@doc doc"""

    get_H!(result::MuseResult, prob::AbstractMuseProblem, [Î¸â‚€=nothing]; kwargs...)

Compute the $H$ matrix, which is part of the MUSE covariance
computation (see [Millea & Seljak,
2021](https://arxiv.org/abs/2112.09354)). 

Positional arguments: 

* `result` â€” `MuseResult` into which to store result
* `prob` â€” `AbstractMuseProblem` being solved
* `Î¸â‚€` â€” the `Î¸` at which to evaluate $H$ (default: `result.Î¸` if it
  exists, otherwise `Î¸â‚€` must be given)

Keyword arguments:

* `zâ‚€` â€” Starting guess for the latent space MAPs. Defaults to random
  sample from prior.
* `âˆ‡z_logLike_atol = 1e-2` â€” Absolute tolerance on the $z$-gradient at
  the MAP solution. 
* `rng` â€” Random number generator to use. Taken from `result.rng` or
  `Random.default_rng()` if not passed. 
* `nsims` â€” How many simulations to average over (default: `10`)
* `pmap` â€” Parallel map function. 
* `progress` â€” Show progress bar (default: `false`), 
* `skip_errors` â€” Ignore any MAP that errors (default: `false`)
* `fdm` â€” A `FiniteDifferenceMethod` used to compute the finite
  difference Jacobian of AD gradients involved in computing $H$
  (defaults to: `FiniteDifferences.central_fdm(3,1)`)
* `step` â€” A guess for the finite difference step-size (defaults to
  0.1Ïƒ for each parameter using J to estimate Ïƒ; for this reason its
  recommended to run `get_J!` before `get_H!`). Is only a guess
  because different choices of `fdm` may adapt this.
* `implicit_diff` â€” Whether to use experimental implicit
  differentiation, rather than finite differences. Will require 2nd
  order AD through your `logLike` so pay close attention to your
  `prob.autodiff`. Either
  `AD.HigherOrderBackend((AD.ForwardDiffBackend(),
  AD.ZygoteBackend()))` or `AD.ForwardDiffBackend()` are recommended
  (default: `false`)

"""
function get_H!(
    result :: MuseResult,
    prob :: AbstractMuseProblem, 
    Î¸â‚€ = result.Î¸;
    fdm :: FiniteDifferenceMethod = central_fdm(3,1),
    âˆ‡z_logLike_atol = 1e-2,
    rng = @something(result.rng, Random.default_rng()),
    nsims = 10, 
    step = nothing, 
    pool = LocalWorkerPool(),
    pmap_over = :auto,
    progress = false,
    skip_errors = false,
    zâ‚€ = nothing,
    implicit_diff = false,
    implicit_diff_cg_kwargs = (maxiter=100, Pl=I),
)

    Î¸â‚€ = standardizeÎ¸(prob, @something(Î¸â‚€, result.Î¸))
    ğŸ˜ = zero(Î¸â‚€) * zero(Î¸â‚€)' # if Î¸::ComponentArray, helps keep component labels 
    nsims_existing = length(result.Hs)
    nsims_remaining = nsims - nsims_existing
    (nsims_remaining <= 0) && return
    
    tâ‚€ = now()

    rngs = split_rng(rng, nsims_remaining)

    pbar = progress ? RemoteProgress(nsims_remaining*(1+length(Î¸â‚€)), 0.1, "get_H: ") : nothing

    # determine if we parallelize over simulations or over columns of
    # the finite-difference jacobian
    if (pmap_over == :jac || ((pmap_over == :auto) && (length(Î¸â‚€) > nsims_remaining)))
        pool_sims, pool_jac = (LocalWorkerPool(), pool)
    else
        pool_sims, pool_jac = (pool, LocalWorkerPool())
    end

    if implicit_diff

        pbar = progress ? RemoteProgress(nsims_remaining, 0.1, "get_H: ") : nothing

        Hs = skipmissing(pmap(pool_sims, rngs) do rng

            try

                (x, z) = sample_x_z(prob, copy(rng), Î¸â‚€)
                zÌ‚, = zÌ‚_at_Î¸(prob, x, 0z, Î¸â‚€, âˆ‡z_logLike_atol=1e-1)
                T = eltype(z)
            
                ad_fwd, ad_rev = AD.second_lowest(prob.autodiff), AD.lowest(prob.autodiff)
            
                ## non-implicit-diff term
                H1 = permutedims(first(AD.jacobian(Î¸â‚€, backend=ad_fwd) do Î¸
                    local x, = sample_x_z(prob, copy(rng), Î¸)
                    first(AD.gradient(Î¸â‚€, backend=ad_rev) do Î¸â€² 
                        logLike(prob, x, zÌ‚, Î¸â€², UnTransformedÎ¸())
                    end)
                end))
            
                ## term involving dzMAP/dÎ¸ via implicit-diff (w/ conjugate-gradient linear solve)
                dFdÎ¸ = first(AD.jacobian(Î¸â‚€, backend=ad_fwd) do Î¸
                    first(AD.gradient(zÌ‚, backend=ad_rev) do z
                        logLike(prob, x, z, Î¸, UnTransformedÎ¸())
                    end)
                end)
                dFdÎ¸1 = first(AD.jacobian(Î¸â‚€, backend=ad_fwd) do Î¸
                    local x, = sample_x_z(prob, copy(rng), Î¸)
                    first(AD.gradient(zÌ‚, backend=ad_rev) do z
                        logLike(prob, x, z, Î¸â‚€, UnTransformedÎ¸())
                    end)
                end)
                # A is the operation of the Hessian of logLike w.r.t. z
                A = LinearMap{T}(length(z), isposdef=true, issymmetric=true, ishermitian=true) do w
                    first(AD.jacobian(0, backend=ad_fwd) do Î±
                        first(AD.gradient(zÌ‚ + Î± * w, backend=ad_rev) do z
                            logLike(prob, x, z, Î¸â‚€, UnTransformedÎ¸())
                        end)
                    end)
                end
                Aâ»Â¹_dFdÎ¸1 = pmap(w -> cg(A, w; implicit_diff_cg_kwargs..., log=true), pool_jac, eachcol(dFdÎ¸1))
                cg_hists = map(last, Aâ»Â¹_dFdÎ¸1)
                H2 = -(dFdÎ¸' * mapreduce(first, hcat, Aâ»Â¹_dFdÎ¸1))

                H = H1 + copyto!(similar(H1), H2)
                progress && ProgressMeter.next!(pbar)
                return H, cg_hists
        
            catch err
                if skip_errors && !(err isa InterruptException)
                    @warn err
                    return (missing,)
                else
                    rethrow(err)
                end
            end

        end)

        append!(result.Hs, map(first, Hs))
        append!(get!(() -> [], result.metadata, :implicit_diff_cg_hists), map(last, Hs))

    else

        # default to finite difference step size of 0.1Ïƒ with Ïƒ roughly
        # estimated from g sims, if we have them
        if step == nothing && !isempty(result.gs)
            step = 0.1 ./ std(result.gs)
        end

        # generate simulations and do initial fit at fiducial, used as
        # starting points for finite difference below
        zÌ‚fids = pmap(pool, rngs) do rngs
            (x, z) = sample_x_z(prob, copy(rng), Î¸â‚€)
            zÌ‚â‚€ = @something(zâ‚€, z)
            zÌ‚, = zÌ‚_at_Î¸(prob, x, zÌ‚â‚€, Î¸â‚€; âˆ‡z_logLike_atol)
            progress && ProgressMeter.next!(pbar)
            zÌ‚
        end

        # finite difference Jacobian
        append!(result.Hs, skipmissing(pmap(pool_sims, zÌ‚fids, rngs) do zÌ‚â‚€, rng
            try
                return first(pjacobian(pool_jac, fdm, Î¸â‚€, step; pbar) do Î¸
                    # sim is generated at Î¸, MAP and gradient are at fiducial Î¸â‚€
                    x, = sample_x_z(prob, copy(rng), Î¸)
                    zÌ‚, = zÌ‚_at_Î¸(prob, x, zÌ‚â‚€, Î¸â‚€; âˆ‡z_logLike_atol)
                    âˆ‡Î¸_logLike(prob, x, zÌ‚, Î¸â‚€, UnTransformedÎ¸())
                end)
            catch err
                if skip_errors && !(err isa InterruptException)
                    @warn err
                    return missing
                else
                    rethrow(err)
                end
            end
        end))

    end
 
    result.H = (Î¸â‚€ isa Number) ? mean(first, result.Hs) : (mean(result.Hs) .+ ğŸ˜)
    result.time += now() - tâ‚€
    finalize_result!(result, prob)

end


@doc doc"""

    get_J!(result::MuseResult, prob::AbstractMuseProblem, [Î¸â‚€=nothing]; kwargs...)

Compute the $J$ matrix, which is part of the MUSE covariance
computation (see [Millea & Seljak,
2021](https://arxiv.org/abs/2112.09354)). 

Positional arguments: 

* `result` â€” `MuseResult` into which to store result
* `prob` â€” `AbstractMuseProblem` being solved
* `Î¸â‚€` â€” the `Î¸` at which to evaluate $J$ (default: `result.Î¸` if it
  exists, otherwise `Î¸â‚€` must be given)

Keyword arguments:

* `zâ‚€` â€” Starting guess for the latent space MAPs. Defaults to random
  sample from prior.
* `âˆ‡z_logLike_atol = 1e-2` â€” Absolute tolerance on the $z$-gradient at
  the MAP solution. 
* `rng` â€” Random number generator to use. Taken from `result.rng` or
  `Random.default_rng()` if not passed. 
* `nsims` â€” How many simulations to average over (default: `100`)
* `pmap` â€” Parallel map function. 
* `progress` â€” Show progress bar (default: `false`), 
* `skip_errors` â€” Ignore any MAP that errors (default: `false`)
* `covariance_method` â€” A `CovarianceEstimator` used to compute $J$
  (default: `SimpleCovariance(corrected=true)`)

"""
function get_J!(
    result :: MuseResult,
    prob :: AbstractMuseProblem, 
    Î¸â‚€ = nothing; 
    zâ‚€ = nothing,
    âˆ‡z_logLike_atol = 1e-2,
    rng = @something(result.rng, Random.default_rng()),
    nsims = 100, 
    pool = LocalWorkerPool(),
    progress = false, 
    skip_errors = false,
    covariance_method = SimpleCovariance(corrected=true),
)

    Î¸â‚€ = standardizeÎ¸(prob, @something(Î¸â‚€, result.Î¸))
    nsims_existing = length(result.gs)
    nsims_remaining = nsims - nsims_existing

    if nsims_remaining > 0

        pbar = progress ? RemoteProgress(nsims_remaining, 0.1, "get_J: ") : nothing

        rngs = split_rng(rng, nsims)[nsims_existing+1:end]

        append!(result.gs, skipmissing(pmap(pool, rngs) do rng
            try
                (x, z) = sample_x_z(prob, rng, Î¸â‚€)
                zÌ‚â‚€ = @something(zâ‚€, z)
                zÌ‚, = zÌ‚_at_Î¸(prob, x, zÌ‚â‚€, Î¸â‚€; âˆ‡z_logLike_atol)
                g = âˆ‡Î¸_logLike(prob, x, zÌ‚, Î¸â‚€, UnTransformedÎ¸())
                progress && ProgressMeter.next!(pbar)
                return g
            catch err
                if skip_errors && !(err isa InterruptException)
                    @warn err
                    return missing
                else
                    rethrow(err)
                end
            end
        end))

    end

    result.J = (Î¸â‚€ isa Number) ? var(result.gs) : cov(covariance_method, identity.(result.gs))
    finalize_result!(result, prob)

end


function finalize_result!(result::MuseResult, prob::AbstractMuseProblem)
    @unpack H, J, Î¸ = result
    if H != nothing && J != nothing && Î¸ != nothing
        ğŸ˜ = zero(J) # if Î¸::ComponentArray, helps keep component labels 
        H_prior = -AD.hessian(AD.ForwardDiffBackend(), Î¸ -> logPriorÎ¸(prob, Î¸, UnTransformedÎ¸()), result.Î¸)[1]
        result.Î£â»Â¹ = H' * inv(J) * H + H_prior + ğŸ˜
        result.Î£ = inv(result.Î£â»Â¹) + ğŸ˜
        if length(result.Î¸) == 1
            result.dist = Normal(result.Î¸[1], sqrt(result.Î£[1]))
        else
            result.dist = MvNormal(result.Î¸, Symmetric(Array(result.Î£)))
        end
    end
    result
end
